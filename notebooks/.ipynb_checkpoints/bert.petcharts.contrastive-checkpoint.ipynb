{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "from util import *\n",
    "from losses import LabelSmoothingCrossEntropy, SupConLoss\n",
    "from augment import *\n",
    "\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "# from torch_model import SupConRobertaNet, SupConMultiRobertaNet\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch_model import MLPRobertaNet, CNNRobertaNet, SIMRobertaNet, ContraRobertaNet\n",
    "from preprop import *\n",
    "from losses import FocalLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.SE_index = [ i for i, c in enumerate(df.columns) if \"SE\" in c][0]\n",
    "        self.label_index = [ i for i, c in enumerate(df.columns) if \"label_id\" in c][0]\n",
    "        self.Num_class = len(df[df.columns[self.label_index]].value_counts())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, self.SE_index]\n",
    "        label = self.df.iloc[idx, self.label_index]\n",
    "        return text, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 14\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "(4280, 5)\n",
      "(476, 5)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('files/train2.csv')\n",
    "test_df = pd.read_csv('files/test2.csv')\n",
    "# train_df = pd.read_csv('data/files/disease_train.csv')\n",
    "# test_df = pd.read_csv('data/files/disease_test.csv')\n",
    "train_df.SE = train_df.SE.apply(lambda x : preprocess(str(x)))\n",
    "test_df.SE = test_df.SE.apply(lambda x : preprocess(str(x)))\n",
    "Num_Label = len(train_df.label_id.value_counts())\n",
    "print(Num_Label)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_info_dict = {16: ' 치주 질환 / 치주염 (젖니 유전자 좌로 유래하는 것 포함)치아 및 구강 질환 ',\n",
    " 12: ' 세균성 장염  소화기 질환 ',\n",
    " 15: ' 췌장염  간 · 담도계 및 췌장 질환',\n",
    " 13: ' 유선 종양 / 유방 종괴  생식기 질환 ',\n",
    " 14: ' 자궁 축농증  생식기 질환 ',\n",
    " 11: ' 판막증 (의심 포함한 심장 잡음 + 심부전 증후 자) 순환기 질환 ',\n",
    " 7: ' 소화관 이물 / 섭취  소화기 질환 ',\n",
    " 10: ' 만성 신장 질환 (신부전 포함)  비뇨기과 질환 ',\n",
    " 3: ' 구토 / 설사 / 혈변 (원인 미정)  소화기 질환 ',\n",
    " 5: ' 방광염  비뇨기과 질환 ',\n",
    " 9: '슬개골 (아) 탈구 근육 골격 질환 ',\n",
    " 1: ' 경련 발작 (원인 미정)  신경 질환 ',\n",
    " 2: ' 고양이 하부 요로 질환 FUS · FLUTD  비뇨기과 질환 ',\n",
    " 17: ' 폐렴  호흡기 질환 ',\n",
    " 0: ' 간 / 담도 / 췌장의 종양  간 · 담도계 및 췌장 질환',\n",
    " 4: ' 당뇨병 내분비 질환 ',\n",
    " 8: ' 수막염 / 수막 뇌염 / 뇌염  신경 질환 ',\n",
    " 6: ' 빈혈 (면역 개입 용혈성) IMHA 혈액 및 조혈기의 질환 '}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContraRobertaNet(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (projection): Linear(in_features=768, out_features=128, bias=True)\n",
       "  (fc): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device('cpu')\n",
    "# pretrained_path = './pretrained_without_wiki'\n",
    "pretrained_path = './pretrained_0818/'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(pretrained_path, do_lower_case=False)\n",
    "# donwstream_class_num = task_label_dict['diags_id']\n",
    "# model = MLPRobertaNet(path=pretrained_path, \n",
    "model = ContraRobertaNet(path=pretrained_path,                       \n",
    "                              embedding_dim=768,\n",
    "                              num_class=Num_Label)\n",
    "model.to(device)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = LabelSmoothingCrossEntropy()\n",
    "# criterion = SupConLoss()\n",
    "# criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reset_parameters(model):\n",
    "#     for p in model.parameters():\n",
    "#         if p.dim() > 1:\n",
    "#             nn.init.xavier_uniform_(p)\n",
    "            \n",
    "# reset_parameters(model)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model.parameters() :\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param, state in zip(model.parameters(), model.state_dict()) :\n",
    "#     print(state)\n",
    "#     print(param.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(test_df) :\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     model.to(device)       \n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = PetDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for text, label in test_loader:\n",
    "        #   encoded_list = [tokenizer.encode(t, add_special_token=True) for t in text]\n",
    "        encoded_list = [tokenizer.encode(t, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(sample=sample, iscontra=False)\n",
    "        logits = outputs\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "\n",
    "    print('Test accuracy: ', total_correct / total_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param, state in zip(model.parameters(), model.state_dict()) :\n",
    "#     print(state)\n",
    "#     print(param.size())\n",
    "# for param, state in zip(model.parameters(), model.state_dict()) :\n",
    "#     print(state)\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_weight(model) :\n",
    "    for param, state in zip(model.parameters(), model.state_dict()) :\n",
    "    #     print(state)\n",
    "#         if 'encoder.pooler.dense.bias' in state:\n",
    "# #             print(param.size())\n",
    "#             print(param[0:10])\n",
    "        if 'encoder.encoder.layer.5.attention.self.query.weight' in state:\n",
    "#             print(param.size())\n",
    "            print(param)\n",
    "#             print(param[0][0:10]) \n",
    "#             print(param[10][0:10]) \n",
    "#             print(param[100][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning_rate=0.00001\n",
    "Learning_rate=0.000004\n",
    "optimizer = Adam(model.parameters(), lr=Learning_rate)\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/4)+1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('contrastive/tune3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/30] Train Loss: 0.4270\n",
      "[Epoch 2/30] Train Loss: 0.4419\n",
      "[Epoch 3/30] Train Loss: 0.4223\n",
      "[Epoch 4/30] Train Loss: 0.4260\n",
      "[Epoch 5/30] Train Loss: 0.4281\n",
      "[Epoch 6/30] Train Loss: 0.4397\n",
      "[Epoch 7/30] Train Loss: 0.4047\n"
     ]
    }
   ],
   "source": [
    "criterion = SupConLoss(temperature=1)\n",
    "# criterion = SupConLoss()\n",
    "criterion = criterion.to(device)\n",
    "model.train()\n",
    "epochs = 30\n",
    "avg_loss = 0.4\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()\n",
    "    for text, label in train_loader:\n",
    "#         print(label)\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list) \n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(sample=sample, iscontra=True)\n",
    "        outputs = torch.unsqueeze(outputs, dim=1)\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "#         print(loss)\n",
    "#         total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "\n",
    "#         print_weight(model)        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "#         if (total_count + 1) % 200 == 0:\n",
    "#             print('###################################')\n",
    "# #             print_weight(model)\n",
    "#             print('Train: [{0}][{1}/{2}]\\t'\n",
    "#                   'loss {loss.avg:.5f}'.format(\n",
    "#                    epoch, total_count + 1, len(train_loader), loss=losses))   \n",
    "\n",
    "    model.train()\n",
    "    scheduler.step()  \n",
    "    print('[Epoch {}/{}] Train Loss: {:.4f}'.format(epoch + 1, epochs, total_loss / total_count))\n",
    "    if total_loss / total_count < avg_loss :\n",
    "        avg_loss = total_loss / total_count\n",
    "        torch.save(model.state_dict(), 'contrastive/tune3')\n",
    "        print('model is saved')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'contrastive/tune2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('contrastive/tune3'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param, state in zip(model.parameters(), model.state_dict()) :\n",
    "    if 'fc.' not in state :\n",
    "        param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/2) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "# criterion = FocalLoss(alpha=0.97, reduce=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion.to(device)\n",
    "epochs = 7\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()\n",
    "    for text, label in train_loader:\n",
    "#         print(label)\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(sample=sample, iscontra=False)\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "#         if (total_count + 1) % 200 == 0:\n",
    "# #             print_weight(model)\n",
    "#             print('Train: [{0}][{1}/{2}]\\t'\n",
    "#                   'loss {loss.avg: .5f}'.format(\n",
    "#                    epoch, total_count + 1, len(train_loader), loss=losses))   \n",
    "\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    model_eval(test_df)\n",
    "    print('[Epoch {}/{}] Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, epochs, total_loss / total_count, total_correct / total_len))\n",
    "#     print(tloss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param, state in zip(model.parameters(), model.state_dict()) :\n",
    "#     if 'fc.' not in state :\n",
    "#         param.requires_grad = True\n",
    "#     else  :\n",
    "#         param.requires_grad = False        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "# scheduler = lr_scheduler.LambdaLR(\n",
    "#     optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/2) + 1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train()\n",
    "# criterion = FocalLoss(alpha=0.97, reduce=True)\n",
    "# # criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion.to(device)\n",
    "# epochs = 7\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     losses = AverageMeter()\n",
    "#     total_loss = 0\n",
    "#     total_len = 0\n",
    "#     total_correct = 0\n",
    "#     total_count = 0\n",
    "#     model.train()\n",
    "#     for text, label in train_loader:\n",
    "# #         print(label)\n",
    "#         encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "#         padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "#         sample = torch.tensor(padded_list)\n",
    "#         sample, label = sample.to(device), label.to(device)\n",
    "#         label = torch.tensor(label)\n",
    "#         outputs = model(sample=sample, iscontra=False)\n",
    "\n",
    "#         loss = criterion(outputs, label)\n",
    "#         losses.update(loss.item(), BATCH_SIZE)\n",
    "# #         print(loss)\n",
    "        \n",
    "# #         total_correct += correct.sum().item()\n",
    "#         total_len += len(label)\n",
    "#         total_loss += loss.item()\n",
    "#         total_count += 1\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step() \n",
    "\n",
    "#         if (total_count + 1) % 200 == 0:\n",
    "# #             print_weight(model)\n",
    "#             print('Train: [{0}][{1}/{2}]\\t'\n",
    "#                   'loss {loss.avg:.5f}'.format(\n",
    "#                    epoch, total_count + 1, len(train_loader), loss=losses))   \n",
    "\n",
    "#     model.train()\n",
    "#     scheduler.step()\n",
    "#     model_eval(test_df)\n",
    "#     print('***********************************')\n",
    "# #     print(tloss)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
