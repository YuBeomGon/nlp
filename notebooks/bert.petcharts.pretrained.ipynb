{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert.petcharts.pretrained.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[],"authorship_tag":"ABX9TyO6VXFF9IYkIBDVgYG4YotR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"1rN2MWC4mrZJ","colab_type":"code","colab":{}},"source":["!pip install transformers\n","!pip install tokenizers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XMeoW8Sfm8WW","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('./gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"67xO0EHHm-V7","colab_type":"code","colab":{}},"source":["cd /content/gdrive/My\\ Drive/DeepLearning"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZqZmmgoVnI5S","colab_type":"code","colab":{}},"source":["from transformers import RobertaConfig\n","from transformers import RobertaTokenizerFast\n","from transformers import RobertaForMaskedLM\n","\n","config = RobertaConfig(\n","    vocab_size=32000,\n","    max_position_embeddings=514,\n","    num_attention_heads=12,\n","    num_hidden_layers=6,\n","    type_vocab_size=1,\n",")\n","\n","tokenizer = RobertaTokenizerFast.from_pretrained(\"./pretrained\", max_len=512)\n","model = RobertaForMaskedLM(config=config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ezRFuiUonMaf","colab_type":"code","colab":{}},"source":["model.num_parameters()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKyw-BVfnN1X","colab_type":"code","colab":{}},"source":["%%time\n","from transformers import LineByLineTextDataset\n","from transformers import DataCollatorForLanguageModeling\n","\n","dataset = LineByLineTextDataset(\n","    tokenizer=tokenizer,\n","    file_path=\"./samples/corpus.txt\",\n","    block_size=128,\n",")\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ObEi4rCgnQ5K","colab_type":"code","colab":{}},"source":["from transformers import Trainer, TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./logs\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=1,\n","    per_gpu_train_batch_size=16,\n","    save_steps=10000,\n","    save_total_limit=2,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=dataset,\n","    prediction_loss_only=True,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UZOlofuQnSPe","colab_type":"code","colab":{}},"source":["%%time\n","trainer.train()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ha7xqvYbnTkZ","colab_type":"code","colab":{}},"source":["trainer.save_model(\"./pretrained\")"],"execution_count":null,"outputs":[]}]}