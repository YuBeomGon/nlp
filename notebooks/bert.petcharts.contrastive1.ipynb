{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vig20H9St5es"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hj9zPQ_YuR3r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "from util import *\n",
    "from losses import LabelSmoothingCrossEntropy, SupConLoss\n",
    "from augment import *\n",
    "\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "# from torch_model import SupConRobertaNet, SupConMultiRobertaNet\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch_model import TransferRobertaNet, ContraRobertaNet\n",
    "from feed import PetDataset\n",
    "from torchsampler import ImbalancedDatasetSampler\n",
    "from losses import FocalLoss\n",
    "from variables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "(62431, 5)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 6\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "train_df = pd.read_csv('files/unlabel_df.csv')\n",
    "# test_df = pd.read_csv('files/test3.csv')\n",
    "Num_Label = len(pd.read_csv('files/train3.csv').label_id.value_counts())\n",
    "print(Num_Label)\n",
    "print(train_df.shape)\n",
    "# print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(np.arange(1,Num_Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict ={}\n",
    "train_df = train_df[['SE', 'label_id']]\n",
    "import sys\n",
    "for label in range(Num_Label) :\n",
    "    df = train_df[train_df['label_id'] == label]\n",
    "    df_dict[label] = df\n",
    "#     print(label, ' : ', len(df))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device('cpu')\n",
    "pretrained_path = './pretrained_without_wiki/'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(pretrained_path, do_lower_case=False)\n",
    "model = ContraRobertaNet(path=pretrained_path,                       \n",
    "                              embedding_dim=768,\n",
    "                              num_class=400)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_clone = TransferRobertaNet(path=pretrained_path,                       \n",
    "#                               embedding_dim=768,\n",
    "#                               num_class=10,\n",
    "#                               num_class1=10)\n",
    "# model_clone.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reset_model(model_clone, model_state=None) :\n",
    "#     if model_state is not None :\n",
    "#         model_clone.load_state_dict(model_state)\n",
    "\n",
    "#     return model_clone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "# indices = list(range(len(train_dataset)))\n",
    "# num_samples = len(indices)\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset, \n",
    "#     sampler=ImbalancedDatasetSampler(train_df, indices=indices, num_samples=num_samples),\n",
    "#     batch_size=12, \n",
    "#     shuffle=False, \n",
    "#     num_workers=2\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_rate = 0.00008\n",
    "optimizer = Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/4) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# criterion = FocalLoss(alpha=0.97, reduce=True)\n",
    "criterion = SupConLoss(temperature=1)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "epochs = 15\n",
    "model.train()\n",
    "high_acc = 0\n",
    "task_batch_size = 6\n",
    "step_size = 2000\n",
    "# state_dict = model_clone.state_dict()\n",
    "\n",
    "# label_list = []\n",
    "# label_list_20 = list(train_df.label_id.value_counts().to_dict().keys())[0:20]\n",
    "# label_list_50 = list(train_df.label_id.value_counts().to_dict().keys())[0:50]\n",
    "# label_list_100 = list(train_df.label_id.value_counts().to_dict().keys())[0:100]\n",
    "# label_list_200 = list(train_df.label_id.value_counts().to_dict().keys())[0:200]\n",
    "# label_list_all = list(train_df.label_id.value_counts().to_dict().keys())\n",
    "# label_list.append(label_list_20)\n",
    "# label_list.append(label_list_50)\n",
    "# label_list.append(label_list_100)\n",
    "# label_list.append(label_list_200)\n",
    "# label_list.append(label_list_all)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()\n",
    "    updates = []\n",
    "    for step in range(0,step_size) :\n",
    "#         labels = random.sample(label_list[step/len(label_list)], BATCH_SIZE)\n",
    "        labels = random.sample(label_list, BATCH_SIZE)\n",
    "        df_list = []\n",
    "        for label in labels :\n",
    "            df_train = df_dict[label].sample(n=2, random_state=(epoch+1)*step, replace=True )\n",
    "#             print(type(df_train))\n",
    "            df_list.append(df_train)\n",
    "        df = pd.concat(df_list)\n",
    "        texts, labels = (df.SE, df.label_id.to_numpy())\n",
    "\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in texts]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        labels = torch.tensor(labels)\n",
    "        sample, labels = sample.to(device), labels.to(device)\n",
    "        outputs = model(sample=sample, iscontra=True)\n",
    "#         print(outputs.size())\n",
    "        outputs = torch.unsqueeze(outputs, dim=1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "        \n",
    "        total_len += len(labels)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        if (total_count) % 500 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'loss {loss.avg:.5f}'.format(\n",
    "                   epoch, total_count, step_size, loss=losses))  \n",
    "    scheduler.step()\n",
    "    torch.save(model.state_dict(), 'maml/pretrain')\n",
    "    print('[Epoch {}/{}] Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, epochs, total_loss / total_count, total_correct / total_len))\n",
    "#     model_eval(test_df, label_tensor)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'maml/pretrain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('files/unlabel_train1.csv')\n",
    "test_df = pd.read_csv('files/unlabel_test1.csv')\n",
    "\n",
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE*2, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df.label_id.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(test_df) :\n",
    "#     device = torch.device(\"cuda\")\n",
    "#     model.to(device)       \n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = PetDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for text, label in test_loader:\n",
    "        #   encoded_list = [tokenizer.encode(t, add_special_token=True) for t in text]\n",
    "        encoded_list = [tokenizer.encode(t, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(sample=sample, iscontra=False)\n",
    "        logits = outputs\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "\n",
    "    print('Test accuracy: ', total_correct / total_len) \n",
    "    return total_correct / total_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning_rate = 0.00004\n",
    "optimizer = Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/4) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = SupConLoss(temperature=1)\n",
    "# criterion = SupConLoss()\n",
    "criterion = criterion.to(device)\n",
    "model.train()\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()\n",
    "    for text, label in train_loader:\n",
    "#         print(label)\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(sample=sample, iscontra=True)\n",
    "        outputs = torch.unsqueeze(outputs, dim=1)\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "#         print(loss)\n",
    "        \n",
    "#         total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "\n",
    "#         print_weight(model)        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        if (total_count) % 50 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'loss {loss.avg:.5f}'.format(\n",
    "                   epoch, total_count, len(train_loader), loss=losses))   \n",
    "\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    print('Learning_rate : ', Learning_rate *1 / (int(epoch/4)+1))\n",
    "#     model_eval(test_df)\n",
    "    print('*************************************************')\n",
    "#     print(tloss)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param, state in zip(model.parameters(), model.state_dict()) :\n",
    "#     print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param, state in zip(model.parameters(), model.state_dict()) :\n",
    "    if 'fc.' not in state :\n",
    "        param.requires_grad = False\n",
    "    else :\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = FocalLoss(alpha=0.97,gamma=1, reduce=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "epochs = 10\n",
    "model.train()\n",
    "high_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()\n",
    "    for text, label in train_loader:\n",
    "#         print(label)\n",
    "\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        labels = torch.tensor(label)\n",
    "        outputs = model(sample=sample, iscontra=False)\n",
    "#         print(outputs.size())\n",
    "#         loss, logits = outputs\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(labels)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "        if total_count % 500 == 0 :\n",
    "            print('loss ', total_loss/total_count)\n",
    "\n",
    "        optimizer.zero_grad()    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    accr = model_eval(test_df)\n",
    "    if accr > high_acc :\n",
    "        high_acc = accr\n",
    "        torch.save(model.state_dict(), 'tune1')\n",
    "\n",
    "    print('[Epoch {}/{}] Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, epochs, total_loss / total_count, total_correct / total_len))\n",
    "# torch.save(model.state_dict(), 'pretrain')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = FocalLoss(alpha=0.97, reduce=True)\n",
    "criterion = SupConLoss(temperature=1)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "epochs = 15\n",
    "model.train()\n",
    "high_acc = 0\n",
    "task_batch_size = 6\n",
    "state_dict = model_clone.state_dict()\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()\n",
    "    updates = []\n",
    "    for text, label in train_loader:\n",
    "        reset_model(model_clone, state_dict)\n",
    "        last_backup = model_clone.state_dict()\n",
    "#         print(label)\n",
    "\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        labels = torch.tensor(label)\n",
    "        outputs = model_clone(sample=sample, istransfer=True)\n",
    "#         print(outputs.size())\n",
    "#         loss, logits = outputs\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(labels)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "        if total_count % 2000 == 0 :\n",
    "            print('loss ', total_loss/total_count)\n",
    "\n",
    "        optimizer.zero_grad()    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        updates.append(subtract_vars(model_clone.state_dict(), last_backup))\n",
    "        if total_count % task_batch_size == 0 :\n",
    "            update = average_vars(updates)\n",
    "            updates = []\n",
    "            state_dict = add_vars(state_dict, scale_vars(update, meta_step_size=0.99))\n",
    "\n",
    "    scheduler.step()\n",
    "#     accr = model_eval(test_df, model)\n",
    "#     if accr > high_acc :\n",
    "#         high_acc = accr\n",
    "#         torch.save(model.state_dict(), 'transfer')\n",
    "#         print('model is saved')\n",
    "\n",
    "    print('[Epoch {}/{}] Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, epochs, total_loss / total_count, total_correct / total_len))\n",
    "# torch.save(model.state_dict(), 'pretrain')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('transfer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('files/train2.csv')\n",
    "test_df = pd.read_csv('files/test2.csv')\n",
    "Label_num = len(train_df.label_id.value_counts())\n",
    "print(Label_num)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.label_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=12, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00002)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/4) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = FocalLoss(alpha=0.97,gamma=1, reduce=True)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "epochs = 10\n",
    "model.train()\n",
    "high_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()\n",
    "    for text, label in train_loader:\n",
    "#         print(label)\n",
    "\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        labels = torch.tensor(label)\n",
    "        outputs = model(sample=sample, istransfer=False)\n",
    "#         print(outputs.size())\n",
    "#         loss, logits = outputs\n",
    "\n",
    "        loss = criterion(outputs, label)\n",
    "\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(labels)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(labels)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "        if total_count % 200 == 0 :\n",
    "            print('loss ', total_loss/total_count)\n",
    "\n",
    "        optimizer.zero_grad()    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    accr = model_eval(test_df, model, istransfer=False)\n",
    "    if accr > high_acc :\n",
    "        high_acc = accr\n",
    "        torch.save(model.state_dict(), 'tune1')\n",
    "\n",
    "    print('[Epoch {}/{}] Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, epochs, total_loss / total_count, total_correct / total_len))\n",
    "# torch.save(model.state_dict(), 'pretrain')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPBrtO3mxP3k2c72K3VwQQk",
   "collapsed_sections": [],
   "name": "bert.petcharts.downstream.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
