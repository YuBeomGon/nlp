{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vig20H9St5es"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hj9zPQ_YuR3r"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2qIUI6OGukuv"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('./gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlN9aoqpu2vW"
   },
   "outputs": [],
   "source": [
    "# cd /content/gdrive/My\\ Drive/DeepLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cub2JrTWu6Se"
   },
   "outputs": [],
   "source": [
    "# train_df = pd.read_csv('./files/train1.csv')\n",
    "# test_df = pd.read_csv('./files/test1.csv')\n",
    "# train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3I-mxXqli4Ri"
   },
   "outputs": [],
   "source": [
    "# train_df = train_df.groupby('label_id').filter(lambda g: len(g) >= 100 and len(g) < 1000)\n",
    "# train_df.groupby('label_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QCZmeQtISlZj"
   },
   "outputs": [],
   "source": [
    "# train_df = train_df.groupby('병원').get_group('D')\n",
    "# train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7gqs4q-vgAr"
   },
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "  def __init__(self, df):\n",
    "    self.df = df\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = self.df.iloc[idx, 2]\n",
    "    label = self.df.iloc[idx, 4]\n",
    "    return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>질병명</th>\n",
       "      <th>주진단코드</th>\n",
       "      <th>SE</th>\n",
       "      <th>진단코드</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>당뇨병</td>\n",
       "      <td>2244</td>\n",
       "      <td>s 다음다뇨 식욕부진 최근들어 다음다뇨 증상과 month 전부터는 식욕도 약간 떨어...</td>\n",
       "      <td>당뇨병</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>간 효소 상승 (원인 미정)</td>\n",
       "      <td>2081</td>\n",
       "      <td>s 건강검진 진행함 혈액검사 간수치 상승 alp alt 모두 상승함 영상검사 신장의...</td>\n",
       "      <td>간질환</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>당뇨병</td>\n",
       "      <td>2244</td>\n",
       "      <td>money 선결제 하셨습니다. 송이 식욕부진 구토 month 전부터 식욕부진 구토...</td>\n",
       "      <td>당뇨병</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>빈혈 (원인 미정)</td>\n",
       "      <td>2231</td>\n",
       "      <td>s 체중감소 건강검진 혈액검사 빈혈 진행중 염증수치 상승 복부초음파검사 담낭내 점액...</td>\n",
       "      <td>빈혈</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>부 신피질 기능 항진증 · 커싱 증후군</td>\n",
       "      <td>2245</td>\n",
       "      <td>털빠짐 켁켁거림 나이가 좀 있어서 추가 검사 원하심 켁켁거리는 건 꽤 오래되었음 y...</td>\n",
       "      <td>신피질 커싱</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      질병명 주진단코드  \\\n",
       "0                    당뇨병   2244   \n",
       "1        간 효소 상승 (원인 미정)   2081   \n",
       "2                    당뇨병   2244   \n",
       "3             빈혈 (원인 미정)   2231   \n",
       "4   부 신피질 기능 항진증 · 커싱 증후군  2245   \n",
       "\n",
       "                                                  SE    진단코드  label_id  \n",
       "0  s 다음다뇨 식욕부진 최근들어 다음다뇨 증상과 month 전부터는 식욕도 약간 떨어...     당뇨병        15  \n",
       "1  s 건강검진 진행함 혈액검사 간수치 상승 alp alt 모두 상승함 영상검사 신장의...     간질환         1  \n",
       "2   money 선결제 하셨습니다. 송이 식욕부진 구토 month 전부터 식욕부진 구토...     당뇨병        15  \n",
       "3  s 체중감소 건강검진 혈액검사 빈혈 진행중 염증수치 상승 복부초음파검사 담낭내 점액...      빈혈        19  \n",
       "4  털빠짐 켁켁거림 나이가 좀 있어서 추가 검사 원하심 켁켁거리는 건 꽤 오래되었음 y...  신피질 커싱        24  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./files/pre-fine.csv')\n",
    "# test_df = pd.read_csv('./files/test1.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device('cpu')\n",
    "# pretrained_path = './pretrained_without_wiki'\n",
    "pretrained_path = './pretrained'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(pretrained_path, do_lower_case=False)\n",
    "model = RobertaForSequenceClassification.from_pretrained(pretrained_path, num_labels=39)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0023,  0.0115, -0.0081,  ...,  0.0004, -0.0018,  0.0028],\n",
      "        [ 0.0031,  0.0073, -0.0066,  ...,  0.0043, -0.0025,  0.0054],\n",
      "        [-0.0105, -0.0065,  0.0073,  ...,  0.0047,  0.0021, -0.0035],\n",
      "        ...,\n",
      "        [ 0.0356, -0.0074,  0.0166,  ...,  0.0478,  0.0349,  0.0278],\n",
      "        [-0.0424,  0.0660, -0.0998,  ...,  0.0166,  0.0948, -0.0435],\n",
      "        [-0.0073,  0.1032, -0.0611,  ...,  0.0290,  0.0966, -0.0325]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(model.state_dict()['roberta.encoder.layer.0.attention.self.query.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / (int(epoch/3) + 1)\n",
    ")\n",
    "epochs = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(test_df) :\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = PetDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for text, label in test_loader:\n",
    "    #   encoded_list = [tokenizer.encode(t, add_special_token=True) for t in text]\n",
    "      encoded_list = [tokenizer.encode(t, max_length=512, truncation=True) for t in text]\n",
    "      padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "      sample = torch.tensor(padded_list)\n",
    "      sample, label = sample.to(device), label.to(device)\n",
    "      labels = torch.tensor(label)\n",
    "      outputs = model(sample, labels=labels)\n",
    "      _, logits = outputs\n",
    "\n",
    "      pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "      correct = pred.eq(labels)\n",
    "      total_correct += correct.sum().item()\n",
    "      total_len += len(labels)\n",
    "\n",
    "    print('Test accuracy: ', total_correct / total_len)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/10] Train Loss: 3.5540, Accuracy: 0.071\n",
      "[Epoch 2/10] Train Loss: 3.4695, Accuracy: 0.078\n",
      "[Epoch 3/10] Train Loss: 3.1790, Accuracy: 0.140\n",
      "[Epoch 4/10] Train Loss: 2.6744, Accuracy: 0.267\n",
      "[Epoch 5/10] Train Loss: 2.3275, Accuracy: 0.361\n",
      "[Epoch 6/10] Train Loss: 2.0013, Accuracy: 0.451\n",
      "[Epoch 7/10] Train Loss: 1.6922, Accuracy: 0.536\n",
      "[Epoch 8/10] Train Loss: 1.4793, Accuracy: 0.604\n",
      "[Epoch 9/10] Train Loss: 1.2969, Accuracy: 0.654\n",
      "[Epoch 10/10] Train Loss: 1.1370, Accuracy: 0.692\n"
     ]
    }
   ],
   "source": [
    "# optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "\n",
    "# scheduler = lr_scheduler.LambdaLR(\n",
    "#     optimizer=optimizer, lr_lambda=lambda epoch: 1 / (int(epoch/3) + 1)\n",
    "# )\n",
    "epochs = 10\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  total_loss = 0\n",
    "  total_len = 0\n",
    "  total_correct = 0\n",
    "  total_count = 0\n",
    "  model.train()\n",
    "  for text, label in train_loader:\n",
    "\n",
    "    encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "    padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "    sample = torch.tensor(padded_list)\n",
    "    sample, label = sample.to(device), label.to(device)\n",
    "    labels = torch.tensor(label)\n",
    "    outputs = model(sample, labels=labels)\n",
    "    loss, logits = outputs\n",
    "\n",
    "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "    correct = pred.eq(labels)\n",
    "    total_correct += correct.sum().item()\n",
    "    total_len += len(labels)\n",
    "    total_loss += loss.item()\n",
    "    total_count += 1\n",
    "    \n",
    "    optimizer.zero_grad()    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  scheduler.step()\n",
    "  model_eval(test_df)\n",
    "\n",
    "  print('[Epoch {}/{}] Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, epochs, total_loss / total_count, total_correct / total_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74840083\n"
     ]
    }
   ],
   "source": [
    "# print(model.num_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./finetune4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0097,  0.0079, -0.0082,  ...,  0.0020, -0.0014,  0.0043],\n",
      "        [ 0.0108,  0.0037, -0.0068,  ...,  0.0056, -0.0020,  0.0066],\n",
      "        [-0.0178, -0.0029,  0.0077,  ...,  0.0040,  0.0020, -0.0049],\n",
      "        ...,\n",
      "        [ 0.0278, -0.0071,  0.0190,  ...,  0.0481,  0.0346,  0.0260],\n",
      "        [-0.0372,  0.0656, -0.0998,  ...,  0.0364,  0.1075, -0.0485],\n",
      "        [-0.0081,  0.1068, -0.0600,  ...,  0.0492,  0.1054, -0.0366]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict()['roberta.encoder.layer.0.attention.self.query.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tune1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0097,  0.0079, -0.0082,  ...,  0.0020, -0.0014,  0.0043],\n",
      "        [ 0.0108,  0.0037, -0.0068,  ...,  0.0056, -0.0020,  0.0066],\n",
      "        [-0.0178, -0.0029,  0.0077,  ...,  0.0040,  0.0020, -0.0049],\n",
      "        ...,\n",
      "        [ 0.0278, -0.0071,  0.0190,  ...,  0.0481,  0.0346,  0.0260],\n",
      "        [-0.0372,  0.0656, -0.0998,  ...,  0.0364,  0.1075, -0.0485],\n",
      "        [-0.0081,  0.1068, -0.0600,  ...,  0.0492,  0.1054, -0.0366]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('tune1'))\n",
    "print(model.state_dict()['roberta.encoder.layer.0.attention.self.query.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RobertaForSequenceClassification.from_pretrained(pretrained_path, num_labels=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'tune1')\n",
    "tune1_dict = { k:v for k, v in torch.load('tune1').items() if 'classifier' not in k}\n",
    "# tune1_dict = {k:v for k,v in torch.load('tune1').items()}\n",
    "# print(type(tune1_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in torch.load('tune2').items() :\n",
    "#     if 'classifier' in k :\n",
    "#         print(k)\n",
    "#         print(v.shape)\n",
    "\n",
    "# #     if tune1_dict[k].shape != tune2_dict[k].shape :\n",
    "#         print(k)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for k, v in model.state_dict() :\n",
    "# #     print(parameter)\n",
    "# model.state_dict()['roberta.encoder.layer.0.attention.self.query.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./pretrained were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ./pretrained and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0023,  0.0115, -0.0081,  ...,  0.0004, -0.0018,  0.0028],\n",
      "        [ 0.0031,  0.0073, -0.0066,  ...,  0.0043, -0.0025,  0.0054],\n",
      "        [-0.0105, -0.0065,  0.0073,  ...,  0.0047,  0.0021, -0.0035],\n",
      "        ...,\n",
      "        [ 0.0356, -0.0074,  0.0166,  ...,  0.0478,  0.0349,  0.0278],\n",
      "        [-0.0424,  0.0660, -0.0998,  ...,  0.0166,  0.0948, -0.0435],\n",
      "        [-0.0073,  0.1032, -0.0611,  ...,  0.0290,  0.0966, -0.0325]])\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = './pretrained'\n",
    "tune2_model = RobertaForSequenceClassification.from_pretrained(pretrained_path, num_labels=10)\n",
    "print(tune2_model.state_dict()['roberta.encoder.layer.0.attention.self.query.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0097,  0.0079, -0.0082,  ...,  0.0020, -0.0014,  0.0043],\n",
      "        [ 0.0108,  0.0037, -0.0068,  ...,  0.0056, -0.0020,  0.0066],\n",
      "        [-0.0178, -0.0029,  0.0077,  ...,  0.0040,  0.0020, -0.0049],\n",
      "        ...,\n",
      "        [ 0.0278, -0.0071,  0.0190,  ...,  0.0481,  0.0346,  0.0260],\n",
      "        [-0.0372,  0.0656, -0.0998,  ...,  0.0364,  0.1075, -0.0485],\n",
      "        [-0.0081,  0.1068, -0.0600,  ...,  0.0492,  0.1054, -0.0366]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "torch.save(tune2_model.state_dict(), 'tune2')\n",
    "tune2_dict = {k:v for k,v in torch.load('tune2').items()}\n",
    "\n",
    "tune2_dict.update(tune1_dict)\n",
    "tune2_model.load_state_dict(tune2_dict)\n",
    "print(tune2_model.state_dict()['roberta.encoder.layer.0.attention.self.query.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "tune2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>질병명</th>\n",
       "      <th>주진단코드</th>\n",
       "      <th>SE</th>\n",
       "      <th>진단코드</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>소화관 이물 / 섭취</td>\n",
       "      <td>2046</td>\n",
       "      <td>month 전에 자두씨 먹음 금 or 토 자두씨 먹은 다음날 회 구토 오늘 오후 ...</td>\n",
       "      <td>소화관 이물</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>소화관 이물 / 섭취</td>\n",
       "      <td>2046</td>\n",
       "      <td>money 선납하셨습니다 그림 cc 구토 입양하셨습니다. 펫샵 본인들이 차 종합백...</td>\n",
       "      <td>소화관 이물</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>판막증 (의심 포함한 심장 잡음 + 심부전 증후 자)</td>\n",
       "      <td>2001</td>\n",
       "      <td>강북동물병원 작년겨울부터 기침증상이 있었던 아이입니다. 단순 감기에 준해 치료를 하...</td>\n",
       "      <td>심장질환</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>소화관 이물 / 섭취</td>\n",
       "      <td>2046</td>\n",
       "      <td>money 선결제하심 정원 초콜렛 섭취. 손바닥만큼의 가나 초콜렛바 하나 다 먹음...</td>\n",
       "      <td>소화관 이물</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>치주 질환 / 치주염 (젖니 유전자 좌로 유래하는 것 포함)</td>\n",
       "      <td>2170</td>\n",
       "      <td>money 선결제 정원 식욕 활력 양호 배변 배뇨 양호 d none 기침 콧물 n...</td>\n",
       "      <td>치주염</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  질병명 주진단코드  \\\n",
       "0                        소화관 이물 / 섭취   2046   \n",
       "1                        소화관 이물 / 섭취   2046   \n",
       "2       판막증 (의심 포함한 심장 잡음 + 심부전 증후 자)  2001   \n",
       "3                        소화관 이물 / 섭취   2046   \n",
       "4   치주 질환 / 치주염 (젖니 유전자 좌로 유래하는 것 포함)  2170   \n",
       "\n",
       "                                                  SE    진단코드  label_id  \n",
       "0   month 전에 자두씨 먹음 금 or 토 자두씨 먹은 다음날 회 구토 오늘 오후 ...  소화관 이물         2  \n",
       "1   money 선납하셨습니다 그림 cc 구토 입양하셨습니다. 펫샵 본인들이 차 종합백...  소화관 이물         2  \n",
       "2  강북동물병원 작년겨울부터 기침증상이 있었던 아이입니다. 단순 감기에 준해 치료를 하...    심장질환         4  \n",
       "3   money 선결제하심 정원 초콜렛 섭취. 손바닥만큼의 가나 초콜렛바 하나 다 먹음...  소화관 이물         2  \n",
       "4   money 선결제 정원 식욕 활력 양호 배변 배뇨 양호 d none 기침 콧물 n...     치주염         9  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('./files/train2.csv')\n",
    "test_df = pd.read_csv('./files/test2.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\")\n",
    "# # device = torch.device('cpu')\n",
    "# pretrained_path = './finetune3'\n",
    "# # pretrained_path = './pretrained'\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(pretrained_path, do_lower_case=False)\n",
    "# model = RobertaForSequenceClassification.from_pretrained(pretrained_path, num_labels=10)\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / (int(epoch/1.5) + 1)\n",
    ")\n",
    "epochs = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.5441176470588235\n",
      "[Epoch 1/10] Train Loss: 2.0548, Accuracy: 0.345\n",
      "Test accuracy:  0.6512605042016807\n",
      "[Epoch 2/10] Train Loss: 1.2418, Accuracy: 0.592\n",
      "Test accuracy:  0.6827731092436975\n",
      "[Epoch 3/10] Train Loss: 0.8260, Accuracy: 0.723\n",
      "Test accuracy:  0.6701680672268907\n",
      "[Epoch 4/10] Train Loss: 0.6061, Accuracy: 0.804\n",
      "Test accuracy:  0.707983193277311\n",
      "[Epoch 5/10] Train Loss: 0.4793, Accuracy: 0.848\n",
      "Test accuracy:  0.6743697478991597\n",
      "[Epoch 6/10] Train Loss: 0.3664, Accuracy: 0.883\n",
      "Test accuracy:  0.6869747899159664\n",
      "[Epoch 7/10] Train Loss: 0.2939, Accuracy: 0.907\n",
      "Test accuracy:  0.6764705882352942\n",
      "[Epoch 8/10] Train Loss: 0.2416, Accuracy: 0.927\n",
      "Test accuracy:  0.6743697478991597\n",
      "[Epoch 9/10] Train Loss: 0.1915, Accuracy: 0.944\n",
      "Test accuracy:  0.6848739495798319\n",
      "[Epoch 10/10] Train Loss: 0.1639, Accuracy: 0.952\n"
     ]
    }
   ],
   "source": [
    "# optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "\n",
    "# scheduler = lr_scheduler.LambdaLR(\n",
    "#     optimizer=optimizer, lr_lambda=lambda epoch: 1 / (int(epoch/3) + 1)\n",
    "# )\n",
    "# epochs = 10\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  total_loss = 0\n",
    "  total_len = 0\n",
    "  total_correct = 0\n",
    "  total_count = 0\n",
    "  model.train()\n",
    "  for text, label in train_loader:\n",
    "\n",
    "    encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "    padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "    sample = torch.tensor(padded_list)\n",
    "    sample, label = sample.to(device), label.to(device)\n",
    "    labels = torch.tensor(label)\n",
    "    outputs = model(sample, labels=labels)\n",
    "    loss, logits = outputs\n",
    "\n",
    "    pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "    correct = pred.eq(labels)\n",
    "    total_correct += correct.sum().item()\n",
    "    total_len += len(labels)\n",
    "    total_loss += loss.item()\n",
    "    total_count += 1\n",
    "    \n",
    "    optimizer.zero_grad()    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  scheduler.step()\n",
    "  model_eval(test_df)\n",
    "\n",
    "  print('[Epoch {}/{}] Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, epochs, total_loss / total_count, total_correct / total_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPBrtO3mxP3k2c72K3VwQQk",
   "collapsed_sections": [],
   "name": "bert.petcharts.downstream.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
