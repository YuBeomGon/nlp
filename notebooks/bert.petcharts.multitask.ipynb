{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "from util import *\n",
    "from losses import SupConLoss\n",
    "from augment import *\n",
    "\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "# from torch_model import SupConRobertaNet, SupConMultiRobertaNet\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "from torch_model import SupConRobertaNet, SupConMultiRobertaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/files/diags_id.csv')\n",
    "# len(df[df.columns[10]].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.SE_index = [ i for i, c in enumerate(df.columns) if \"SE\" in c][0]\n",
    "        self.label_index = [ i for i, c in enumerate(df.columns) if \"label_id\" in c][0]\n",
    "        self.Num_class = len(df[df.columns[self.label_index]].value_counts())\n",
    "        if len([ i for i, c in enumerate(df.columns) if \"task_id\" in c]) > 0:\n",
    "            self.task_index = [ i for i, c in enumerate(df.columns) if \"task_id\" in c][0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, self.SE_index]\n",
    "        label = self.df.iloc[idx, self.label_index]\n",
    "        task_id = self.df.iloc[idx, self.task_index]\n",
    "        return text, label, task_id\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diags_id_1st.csv\n",
      "9\n",
      "symptoms_id_1st.csv\n",
      "7\n",
      "diags_id_2nd.csv\n",
      "11\n",
      "disease_id_1st.csv\n",
      "5\n",
      "diags_id_3rd.csv\n",
      "30\n",
      "symptoms_id_3rd.csv\n",
      "21\n",
      "disease_id_2nd.csv\n",
      "10\n",
      "symptoms_id_2nd.csv\n",
      "12\n",
      "diags_id_4th.csv\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "task_ids = ['diags_id_1st','diags_id_2nd','diags_id_3rd','diags_id_4th',\n",
    "            'disease_id_1st','disease_id_2nd','symptoms_id_1st','symptoms_id_2nd','symptoms_id_3rd']\n",
    "random.shuffle(task_ids)\n",
    "task_label_dict = {}\n",
    "dataset_all = []\n",
    "# df_all = []\n",
    "# files = [ f for f in os.listdir('data/files') if 'id' in f]\n",
    "files = [ f + '.csv' for f in  task_ids]\n",
    "for file in files :\n",
    "    print(file)\n",
    "    df = pd.read_csv('data/files' + '/' + file)\n",
    "    label_index = [ i for i, c in enumerate(df.columns) if \"label_id\" in c][0]\n",
    "    print(len(df[df.columns[label_index]].value_counts()))\n",
    "    df['task_id'] = file.split('.')[0]\n",
    "    task_label_dict[file.split('.')[0]] = len(df[df.columns[label_index]].value_counts())\n",
    "    df.dropna(subset=['SE'], inplace=True)\n",
    "    dataset_all.append(PetDataset(df))\n",
    "\n",
    "concat_dataset = ConcatDataset(dataset_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'diags_id_2nd': 11, 'disease_id_2nd': 10, 'diags_id_3rd': 30, 'symptoms_id_3rd': 21, 'symptoms_id_2nd': 12, 'disease_id_1st': 5, 'symptoms_id_1st': 7, 'diags_id_1st': 9, 'diags_id_4th': 40}\n"
     ]
    }
   ],
   "source": [
    "print(task_label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchSchedulerSampler(torch.utils.data.sampler.Sampler):\n",
    "    \"\"\"\n",
    "    iterate over tasks and provide a random batch per task in each mini-batch\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.number_of_datasets = len(dataset.datasets)\n",
    "        self.largest_dataset_size = max([len(cur_dataset) for cur_dataset in dataset.datasets])    \n",
    "\n",
    "    def __len__(self):\n",
    "            return self.batch_size * math.ceil(self.largest_dataset_size / self.batch_size) * len(self.dataset.datasets)\n",
    "\n",
    "    def __iter__(self):\n",
    "        samplers_list = []\n",
    "        sampler_iterators = []\n",
    "        for dataset_idx in range(self.number_of_datasets):\n",
    "            cur_dataset = self.dataset.datasets[dataset_idx]\n",
    "            sampler = RandomSampler(cur_dataset)\n",
    "            samplers_list.append(sampler)\n",
    "            cur_sampler_iterator = sampler.__iter__()\n",
    "            sampler_iterators.append(cur_sampler_iterator)   \n",
    "            \n",
    "        push_index_val = [0] + self.dataset.cumulative_sizes[:-1]\n",
    "        step = self.batch_size * self.number_of_datasets\n",
    "        samples_to_grab = self.batch_size\n",
    "        # for this case we want to get all samples in dataset, this force us to resample from the smaller datasets\n",
    "        epoch_samples = self.largest_dataset_size * self.number_of_datasets\n",
    "\n",
    "        final_samples_list = []  # this is a list of indexes from the combined dataset\n",
    "        for _ in range(0, epoch_samples, step):\n",
    "            for i in range(self.number_of_datasets):\n",
    "                cur_batch_sampler = sampler_iterators[i]\n",
    "                cur_samples = []\n",
    "                for _ in range(samples_to_grab):\n",
    "                    try:\n",
    "                        cur_sample_org = cur_batch_sampler.__next__()\n",
    "                        cur_sample = cur_sample_org + push_index_val[i]\n",
    "                        cur_samples.append(cur_sample)\n",
    "                    except StopIteration:\n",
    "                        # got to the end of iterator - restart the iterator and continue to get samples\n",
    "                        # until reaching \"epoch_samples\"\n",
    "                        sampler_iterators[i] = samplers_list[i].__iter__()\n",
    "                        cur_batch_sampler = sampler_iterators[i]\n",
    "                        cur_sample_org = cur_batch_sampler.__next__()\n",
    "                        cur_sample = cur_sample_org + push_index_val[i]\n",
    "                        cur_samples.append(cur_sample)\n",
    "                final_samples_list.extend(cur_samples)\n",
    "\n",
    "        return iter(final_samples_list)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(test_df, ContraMutliFlag=2) :\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = PetDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for text, label, task_id in test_loader:\n",
    "        #   encoded_list = [tokenizer.encode(t, add_special_token=True) for t in text]\n",
    "        encoded_list = [tokenizer.encode(t, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "#         outputs = model(ContraMutliFlag=3, task_id=(task_id[0]), sample=sample)\n",
    "        outputs = model(ContraMutliFlag=ContraMutliFlag, task_id='downstream', sample=sample)\n",
    "        logits = outputs\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "\n",
    "    print('Test accuracy: ', total_correct / total_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'diags_id_2nd': 11,\n",
       " 'disease_id_2nd': 10,\n",
       " 'diags_id_3rd': 30,\n",
       " 'symptoms_id_3rd': 21,\n",
       " 'symptoms_id_2nd': 12,\n",
       " 'disease_id_1st': 5,\n",
       " 'symptoms_id_1st': 7,\n",
       " 'diags_id_1st': 9,\n",
       " 'diags_id_4th': 40}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('files/train3.csv')\n",
    "train_df['task_id'] = 'downstream'\n",
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)\n",
    "donwstream_class_num = train_dataset.Num_class\n",
    "print(donwstream_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device('cpu')\n",
    "# pretrained_path = './pretrained_without_wiki'\n",
    "pretrained_path = './pretrained_without_wiki/'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(pretrained_path, do_lower_case=False)\n",
    "# donwstream_class_num = task_label_dict['diags_id']\n",
    "model = SupConMultiRobertaNet(path=pretrained_path, \n",
    "                              embedding_dim=768,\n",
    "                              feat_dim=64,\n",
    "                              task_label_dict=task_label_dict, \n",
    "                              num_class=donwstream_class_num)\n",
    "model.to(device)\n",
    "criterion = SupConLoss(temperature=0.07)\n",
    "criterion1 = torch.nn.CrossEntropyLoss()\n",
    "criterion1 = criterion1.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eval(test_df, ContraMutliFlag=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/4) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic dataloader\n",
    "import sys\n",
    "# dataloader = DataLoader(dataset=concat_dataset,\n",
    "#                          batch_size=BATCH_SIZE,\n",
    "#                          shuffle=True)\n",
    "\n",
    "# dataloader with BatchSchedulerSampler\n",
    "dataloader = DataLoader(dataset=concat_dataset,\n",
    "                         sampler=BatchSchedulerSampler(dataset=concat_dataset,\n",
    "                                                       batch_size=BATCH_SIZE),\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def one_iteration(text, label, task_id) :    \n",
    "#     encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "#     padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "#     sample = torch.tensor(padded_list)\n",
    "#     sample, label = sample.to(device), label.to(device)\n",
    "#     label = torch.tensor(label)\n",
    "#     outputs = model(ContraMutliFlag=2, task_id=task_id, sample=sample)\n",
    "    \n",
    "#     loss = criterion1(outputs, label)\n",
    "\n",
    "#     pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "#     correct = pred.eq(label)\n",
    "# #     total_correct += correct.sum().item()\n",
    "# #     total_len += len(labels)\n",
    "# #     total_loss += loss.item()\n",
    "# #     total_count += 1\n",
    "    \n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step() \n",
    "    \n",
    "#     return loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "#     model.train()    \n",
    "    for text, label, task_id in dataloader:\n",
    "#         print(text)\n",
    "#         print(label)\n",
    "#         print(task_id[0])\n",
    "#         sys.exit()\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(ContraMutliFlag=2, task_id=task_id[0], sample=sample)\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        loss = criterion1(outputs, label)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "#         print(loss)\n",
    "        \n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        if (total_count + 1) % 100 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'loss {loss.avg:.5f}'.format(\n",
    "                   epoch, total_count + 1, len(dataloader), loss=losses))   \n",
    "\n",
    "    model.train()\n",
    "    scheduler.step()    \n",
    "    print('***********************************')\n",
    "#     print(tloss)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'finetune/multitasked'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name in (model.state_dict()) :\n",
    "#     print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in zip(model.state_dict(), model.parameters()) :\n",
    "    if 'fc.' not in name :\n",
    "        param.requires_grad = False\n",
    "#     print(name)\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(813, 16)\n",
      "(813, 16)\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('files/test3.csv')\n",
    "test_df['task_id'] = 'downstream'\n",
    "print(test_df.shape)\n",
    "# test_df.SE = test_df.SE.apply(lambda x : np.nan if x == \"Nan\" else x)\n",
    "test_df.dropna(subset=['SE'], inplace=True)\n",
    "print(test_df.shape)\n",
    "# test_df.diags_id = test_df.diags_id.apply(lambda x : int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00006)\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/4) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0][100/585]\tloss 0.42408\n",
      "Train: [0][200/585]\tloss 0.42219\n",
      "Train: [0][300/585]\tloss 0.41543\n",
      "Train: [0][400/585]\tloss 0.40937\n",
      "Train: [0][500/585]\tloss 0.41571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.6519065190651907\n",
      "Train: [1][100/585]\tloss 0.30868\n",
      "Train: [1][200/585]\tloss 0.32071\n",
      "Train: [1][300/585]\tloss 0.31875\n",
      "Train: [1][400/585]\tloss 0.32189\n",
      "Train: [1][500/585]\tloss 0.32247\n",
      "Test accuracy:  0.6494464944649446\n",
      "Train: [2][100/585]\tloss 0.23931\n",
      "Train: [2][200/585]\tloss 0.24318\n",
      "Train: [2][300/585]\tloss 0.24904\n",
      "Train: [2][400/585]\tloss 0.25221\n",
      "Train: [2][500/585]\tloss 0.24680\n",
      "Test accuracy:  0.6691266912669127\n"
     ]
    }
   ],
   "source": [
    "# downstream task\n",
    "epochs = 3\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()    \n",
    "    for text, label, task_id in train_loader:\n",
    "#         print(task_id)\n",
    "#         print(text)\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(ContraMutliFlag=3, task_id=task_id[0], sample=sample)\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        loss = criterion1(outputs, label)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "        \n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        if (total_count + 1) % 100 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'loss {loss.avg:.5f}'.format(\n",
    "                   epoch, total_count + 1, len(train_loader), loss=losses))   \n",
    "        \n",
    "    scheduler.step()\n",
    "    model_eval(test_df, ContraMutliFlag=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in zip(model.state_dict(), model.parameters()) :\n",
    "#     if 'fc.' not in name :\n",
    "    param.requires_grad = True\n",
    "#     print(name)\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00001)\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/4) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0][100/585]\tloss 0.94873\n",
      "Train: [0][200/585]\tloss 0.98815\n",
      "Train: [0][300/585]\tloss 1.00950\n",
      "Train: [0][400/585]\tloss 0.99730\n",
      "Train: [0][500/585]\tloss 0.99981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.45264452644526443\n",
      "Train: [1][100/585]\tloss 0.97639\n",
      "Train: [1][200/585]\tloss 0.93414\n",
      "Train: [1][300/585]\tloss 0.91222\n",
      "Train: [1][400/585]\tloss 0.91201\n",
      "Train: [1][500/585]\tloss 0.91100\n",
      "Test accuracy:  0.45264452644526443\n",
      "Train: [2][100/585]\tloss 0.85521\n",
      "Train: [2][200/585]\tloss 0.85377\n",
      "Train: [2][300/585]\tloss 0.84502\n",
      "Train: [2][400/585]\tloss 0.85407\n",
      "Train: [2][500/585]\tloss 0.85727\n",
      "Test accuracy:  0.45141451414514144\n",
      "Train: [3][100/585]\tloss 0.84787\n",
      "Train: [3][200/585]\tloss 0.83428\n",
      "Train: [3][300/585]\tloss 0.82715\n",
      "Train: [3][400/585]\tloss 0.81493\n",
      "Train: [3][500/585]\tloss 0.83187\n",
      "Test accuracy:  0.46248462484624847\n",
      "Train: [4][100/585]\tloss 0.80919\n",
      "Train: [4][200/585]\tloss 0.80257\n",
      "Train: [4][300/585]\tloss 0.79508\n",
      "Train: [4][400/585]\tloss 0.78590\n",
      "Train: [4][500/585]\tloss 0.78304\n",
      "Test accuracy:  0.46002460024600245\n",
      "Train: [5][100/585]\tloss 0.78508\n",
      "Train: [5][200/585]\tloss 0.78564\n",
      "Train: [5][300/585]\tloss 0.77301\n",
      "Train: [5][400/585]\tloss 0.75899\n",
      "Train: [5][500/585]\tloss 0.76653\n",
      "Test accuracy:  0.45879458794587946\n",
      "Train: [6][100/585]\tloss 0.74926\n",
      "Train: [6][200/585]\tloss 0.73772\n",
      "Train: [6][300/585]\tloss 0.75528\n",
      "Train: [6][400/585]\tloss 0.75539\n",
      "Train: [6][500/585]\tloss 0.74836\n",
      "Test accuracy:  0.44772447724477243\n",
      "Train: [7][100/585]\tloss 0.75829\n",
      "Train: [7][200/585]\tloss 0.73524\n",
      "Train: [7][300/585]\tloss 0.72486\n",
      "Train: [7][400/585]\tloss 0.73538\n",
      "Train: [7][500/585]\tloss 0.73237\n",
      "Test accuracy:  0.45510455104551045\n",
      "Train: [8][100/585]\tloss 0.70081\n",
      "Train: [8][200/585]\tloss 0.68453\n",
      "Train: [8][300/585]\tloss 0.70483\n",
      "Train: [8][400/585]\tloss 0.70116\n",
      "Train: [8][500/585]\tloss 0.70731\n",
      "Test accuracy:  0.45879458794587946\n",
      "Train: [9][100/585]\tloss 0.70267\n",
      "Train: [9][200/585]\tloss 0.71561\n",
      "Train: [9][300/585]\tloss 0.71671\n",
      "Train: [9][400/585]\tloss 0.68996\n",
      "Train: [9][500/585]\tloss 0.70143\n",
      "Test accuracy:  0.45141451414514144\n"
     ]
    }
   ],
   "source": [
    "# downstream task\n",
    "epochs = 10\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()    \n",
    "    for text, label, task_id in train_loader:\n",
    "#         print(task_id)\n",
    "#         print(text)\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(ContraMutliFlag=3, task_id=task_id[0], sample=sample)\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        loss = criterion1(outputs, label)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "        \n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        if (total_count + 1) % 100 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'loss {loss.avg:.5f}'.format(\n",
    "                   epoch, total_count + 1, len(train_loader), loss=losses))   \n",
    "        \n",
    "    scheduler.step()\n",
    "    model_eval(test_df, ContraMutliFlag=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00002)\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/2) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0][100/585]\tloss 0.75904\n",
      "Train: [0][200/585]\tloss 0.72472\n",
      "Train: [0][300/585]\tloss 0.71838\n",
      "Train: [0][400/585]\tloss 0.71168\n",
      "Train: [0][500/585]\tloss 0.70548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.45264452644526443\n",
      "Train: [1][100/585]\tloss 0.67009\n",
      "Train: [1][200/585]\tloss 0.65710\n",
      "Train: [1][300/585]\tloss 0.65178\n",
      "Train: [1][400/585]\tloss 0.65292\n",
      "Train: [1][500/585]\tloss 0.65085\n",
      "Test accuracy:  0.46863468634686345\n",
      "Train: [2][100/585]\tloss 0.61587\n",
      "Train: [2][200/585]\tloss 0.57734\n",
      "Train: [2][300/585]\tloss 0.60297\n",
      "Train: [2][400/585]\tloss 0.60041\n",
      "Train: [2][500/585]\tloss 0.59767\n",
      "Test accuracy:  0.46248462484624847\n",
      "Train: [3][100/585]\tloss 0.58885\n",
      "Train: [3][200/585]\tloss 0.56309\n",
      "Train: [3][300/585]\tloss 0.56188\n",
      "Train: [3][400/585]\tloss 0.56236\n",
      "Train: [3][500/585]\tloss 0.56195\n",
      "Test accuracy:  0.45387453874538747\n",
      "Train: [4][100/585]\tloss 0.52531\n",
      "Train: [4][200/585]\tloss 0.51730\n",
      "Train: [4][300/585]\tloss 0.53436\n",
      "Train: [4][400/585]\tloss 0.53223\n",
      "Train: [4][500/585]\tloss 0.53697\n",
      "Test accuracy:  0.46002460024600245\n",
      "Train: [5][100/585]\tloss 0.53618\n",
      "Train: [5][200/585]\tloss 0.54476\n",
      "Train: [5][300/585]\tloss 0.52584\n",
      "Train: [5][400/585]\tloss 0.52598\n",
      "Train: [5][500/585]\tloss 0.51527\n",
      "Test accuracy:  0.45633456334563344\n",
      "Train: [6][100/585]\tloss 0.51979\n",
      "Train: [6][200/585]\tloss 0.51360\n",
      "Train: [6][300/585]\tloss 0.49917\n",
      "Train: [6][400/585]\tloss 0.49893\n",
      "Train: [6][500/585]\tloss 0.49650\n",
      "Test accuracy:  0.45510455104551045\n",
      "Train: [7][100/585]\tloss 0.54583\n",
      "Train: [7][200/585]\tloss 0.52325\n",
      "Train: [7][300/585]\tloss 0.50785\n",
      "Train: [7][400/585]\tloss 0.49098\n",
      "Train: [7][500/585]\tloss 0.48510\n",
      "Test accuracy:  0.46494464944649444\n",
      "Train: [8][100/585]\tloss 0.50150\n",
      "Train: [8][200/585]\tloss 0.49169\n",
      "Train: [8][300/585]\tloss 0.48569\n",
      "Train: [8][400/585]\tloss 0.48578\n",
      "Train: [8][500/585]\tloss 0.47982\n",
      "Test accuracy:  0.45510455104551045\n",
      "Train: [9][100/585]\tloss 0.47131\n",
      "Train: [9][200/585]\tloss 0.48779\n",
      "Train: [9][300/585]\tloss 0.47131\n",
      "Train: [9][400/585]\tloss 0.46392\n",
      "Train: [9][500/585]\tloss 0.46012\n",
      "Test accuracy:  0.46002460024600245\n"
     ]
    }
   ],
   "source": [
    "# downstream task\n",
    "epochs = 10\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()    \n",
    "    for text, label, task_id in train_loader:\n",
    "#         print(task_id)\n",
    "#         print(text)\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(ContraMutliFlag=3, task_id=task_id[0], sample=sample)\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        loss = criterion1(outputs, label)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "        \n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        \n",
    "        if (total_count + 1) % 100 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'loss {loss.avg:.5f}'.format(\n",
    "                   epoch, total_count + 1, len(train_loader), loss=losses))   \n",
    "        \n",
    "    scheduler.step()\n",
    "    model_eval(test_df, ContraMutliFlag=3)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
