{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "from util import *\n",
    "from losses import SupConLoss\n",
    "from augment import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupConRobertaNet(nn.Module):\n",
    "    \"\"\"backbone + projection head\"\"\"\n",
    "    def __init__(self, embedding_dim=768, feat_dim=64, num_class=10):\n",
    "        super(SupConRobertaNet, self).__init__()\n",
    "#         model_fun, dim_in = model_dict[name]\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        self.num_class = num_class\n",
    "        print(num_class)\n",
    "        self.encoder = RobertaModel.from_pretrained('notebooks/pretrained_without_wiki/')\n",
    "#         self.encoder = model_fun()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(self.embedding_dim, self.embedding_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.embedding_dim, self.feat_dim)\n",
    "        )\n",
    "        self.fc =  nn.Linear(self.embedding_dim, self.num_class)\n",
    "\n",
    "    def forward(self, iscontra, x ):\n",
    "        if iscontra == True :\n",
    "            r = self.encoder(x)\n",
    "            r = r[0][:,0,:]\n",
    "            z = F.normalize(self.projection(r), dim=1)\n",
    "            return z\n",
    "        else :\n",
    "            r = self.encoder(x)\n",
    "            r = r[0][:,0,:]\n",
    "            r = self.fc(r)\n",
    "            return r\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, 2]\n",
    "        label = self.df.iloc[idx, 4]\n",
    "#         text = self.df.text\n",
    "#         lable = self.df.label_id\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('notebooks/files/train2.csv')\n",
    "# val_df = pd.read_csv('files/val.csv')\n",
    "test_df = pd.read_csv('notebooks/files/test2.csv')\n",
    "# Num_class = len(set(train_df.label.value_counts()))\n",
    "Num_class = 10\n",
    "print(Num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df.label_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('notebooks/pretrained_without_wiki/', do_lower_case=False)\n",
    "model = SupConRobertaNet(num_class=Num_class)\n",
    "# model = AlbertModel.from_pretrained('albert-base-v2')\n",
    "# print(model)\n",
    "criterion = SupConLoss(temperature=0.07)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "test_dataset = PetDataset(test_df)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARN_RATE = 0.00001\n",
    "optimizer = Adam(\n",
    "    model.parameters(), lr=LEARN_RATE, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01\n",
    ")\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/2) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # supervised contrastive learning\n",
    "# # import sys\n",
    "# model.train()\n",
    "# epochs = 5\n",
    "# # for index, parameter in enumerate(model.encoder.parameters()):\n",
    "# #     if index < 2:\n",
    "# # #         print(parameter.size())\n",
    "# #         print(parameter.data[0][0])  \n",
    "\n",
    "# for epoch in range(1, epochs +1) :\n",
    "#     losses = AverageMeter()\n",
    "#     count = 0\n",
    "#     tloss = 0\n",
    "#     for texts, labels in train_loader:\n",
    "# #         print(labels)\n",
    "#         aug_texts = []\n",
    "#         for text in texts :\n",
    "#             aug_text = text_aug(text)\n",
    "#             aug_texts.append(aug_text)\n",
    "            \n",
    "#         encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=MAX_SEQ_LEN, truncation=True, padding=True) for t in texts]\n",
    "#         aug_encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=MAX_SEQ_LEN, truncation=True, padding=True) for t in aug_texts]\n",
    "#         padded_list = [e[:MAX_SEQ_LEN] + [0] * (MAX_SEQ_LEN - len(e[:MAX_SEQ_LEN])) for e in encoded_list]\n",
    "#         aug_padded_list = [e[:MAX_SEQ_LEN] + [0] * (MAX_SEQ_LEN - len(e[:MAX_SEQ_LEN])) for e in aug_encoded_list]\n",
    "#         sample = torch.tensor(padded_list)\n",
    "#         aug_sample = torch.tensor(aug_padded_list)\n",
    "#         samples = torch.cat([sample, aug_sample], dim=0)     \n",
    "#         samples, labels = samples.to(device), labels.to(device)\n",
    "        \n",
    "#         labels = torch.tensor(labels)\n",
    "#         batch_size = labels.shape[0]\n",
    "#         outputs = model(True, samples) # projection layer\n",
    "# #         print(torch.matmul(outputs, outputs.T))\n",
    "        \n",
    "#         z1, z2 = torch.split(outputs, [batch_size, batch_size], dim=0)\n",
    "#         features = torch.cat([z1.unsqueeze(1), z2.unsqueeze(1)], dim=1)\n",
    "# #         features = outputs.unsqueeze(1)\n",
    "        \n",
    "#         loss = criterion(features, labels)\n",
    "#         losses.update(loss.item(), batch_size)\n",
    "# #         print(loss.item())  \n",
    "#         # print info\n",
    "#         if (count + 1) % 100 == 0:\n",
    "#             print('Train: [{0}][{1}/{2}]\\t'\n",
    "#                   'loss {loss.val:.5f} ({loss.avg:.5f})'.format(\n",
    "#                    epoch, count + 1, len(train_loader), loss=losses))        \n",
    "#         tloss += loss.item()\n",
    "#         count += 1\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     scheduler.step()\n",
    "        \n",
    "#     print('***********************************')\n",
    "#     print(tloss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised contrastive learning\n",
    "# import sys\n",
    "model.train()\n",
    "epochs = 5\n",
    "for epoch in range(1, epochs +1) :\n",
    "    losses = AverageMeter()\n",
    "    count = 0\n",
    "    tloss = 0\n",
    "    for texts, labels in train_loader:\n",
    "#         print(labels)\n",
    "            \n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=MAX_SEQ_LEN, truncation=True, padding=True) for t in texts]\n",
    "        padded_list = [e[:MAX_SEQ_LEN] + [0] * (MAX_SEQ_LEN - len(e[:MAX_SEQ_LEN])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)    \n",
    "        sample, labels = sample.to(device), labels.to(device)\n",
    "        \n",
    "        labels = torch.tensor(labels)\n",
    "        batch_size = labels.shape[0]\n",
    "        outputs = model(True, sample) # projection layer\n",
    "#         print(torch.matmul(outputs, outputs.T))\n",
    "        features = outputs.unsqueeze(1)\n",
    "        \n",
    "        loss = criterion(features, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "#         print(loss.item())  \n",
    "        # print info\n",
    "        if (count + 1) % 100 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'loss {loss.val:.5f} ({loss.avg:.5f})'.format(\n",
    "                   epoch, count + 1, len(train_loader), loss=losses))        \n",
    "        tloss += loss.item()\n",
    "        count += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "        \n",
    "    print('***********************************')\n",
    "    print(tloss) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(test_df) :\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = PetDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for text, label in test_loader:\n",
    "    #   encoded_list = [tokenizer.encode(t, add_special_token=True) for t in text]\n",
    "      encoded_list = [tokenizer.encode(t, max_length=512, truncation=True) for t in text]\n",
    "      padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "      sample = torch.tensor(padded_list)\n",
    "      sample, label = sample.to(device), label.to(device)\n",
    "      label = torch.tensor(label)\n",
    "      outputs = model(False, sample)\n",
    "      logits = outputs\n",
    "\n",
    "      pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "      correct = pred.eq(label)\n",
    "      total_correct += correct.sum().item()\n",
    "      total_len += len(label)\n",
    "\n",
    "    print('Test accuracy: ', total_correct / total_len)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=LEARN_RATE/5)\n",
    "\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / (int(epoch/3) + 1)\n",
    ")\n",
    "epochs = 5\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/beomgon2/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.15546218487394958\n",
      "[Epoch 1/10] Train Loss: 2.2875, Accuracy: 0.127\n",
      "Test accuracy:  0.17647058823529413\n",
      "[Epoch 2/10] Train Loss: 2.2744, Accuracy: 0.136\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-36cb94c53ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/vtdeep/petcharts/pet-env/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "MAX_SEQ_LEN = 512\n",
    "model.train()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  total_loss = 0\n",
    "  total_len = 0\n",
    "  total_correct = 0\n",
    "  total_count = 0\n",
    "  model.train()\n",
    "  for text, label in train_loader:           \n",
    "    encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=MAX_SEQ_LEN, truncation=True, padding=True) for t in texts]\n",
    "    padded_list = [e[:MAX_SEQ_LEN] + [0] * (MAX_SEQ_LEN - len(e[:MAX_SEQ_LEN])) for e in encoded_list]\n",
    "    sample = torch.tensor(padded_list)  \n",
    "    sample, label = sample.to(device), label.to(device)\n",
    "\n",
    "    label = torch.tensor(label)\n",
    "    batch_size = label.shape[0]\n",
    "    outputs = model(False, sample) # classifier layer\n",
    "#     print(outputs.size())\n",
    "\n",
    "    loss = criterion(outputs, label)\n",
    "    losses.update(loss.item(), batch_size)\n",
    "    pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "    correct = pred.eq(label)\n",
    "#     print(label)\n",
    "#     print(correct)\n",
    "\n",
    "    total_correct += correct.sum().item()\n",
    "    total_len += len(label)\n",
    "    total_loss += loss.item()\n",
    "    total_count += 1        \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()  \n",
    "  \n",
    "  scheduler.step()\n",
    "  model_eval(test_df)\n",
    "#   model_eval(test_df)\n",
    "\n",
    "  print('[Epoch {}/{}] Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch + 1, epochs, total_loss / total_count, total_correct / total_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
