{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, RobertaModel\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import torch.nn as nn\n",
    "\n",
    "from util import *\n",
    "from losses import LabelSmoothingCrossEntropy\n",
    "from augment import *\n",
    "\n",
    "from torch.utils.data.dataset import ConcatDataset\n",
    "# from torch_model import SupConRobertaNet, SupConMultiRobertaNet\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "from torch_model import MLPRobertaNet, CNNRobertaNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PetDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.SE_index = [ i for i, c in enumerate(df.columns) if \"SE\" in c][0]\n",
    "        self.label_index = [ i for i, c in enumerate(df.columns) if \"label_id\" in c][0]\n",
    "        self.Num_class = len(df[df.columns[self.label_index]].value_counts())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx, self.SE_index]\n",
    "        label = self.df.iloc[idx, self.label_index]\n",
    "        return text, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "MAX_SEQ_LEN = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "(4674, 14)\n",
      "(813, 15)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('files/train3.csv')\n",
    "test_df = pd.read_csv('files/test3.csv')\n",
    "Num_Label = len(train_df.label_id.value_counts())\n",
    "print(Num_Label)\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PetDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "# device = torch.device('cpu')\n",
    "# pretrained_path = './pretrained_without_wiki'\n",
    "pretrained_path = './pretrained_without_wiki/'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(pretrained_path, do_lower_case=False)\n",
    "# donwstream_class_num = task_label_dict['diags_id']\n",
    "# model = MLPRobertaNet(path=pretrained_path, \n",
    "model = CNNRobertaNet(path=pretrained_path,                       \n",
    "                              embedding_dim=768,\n",
    "                              max_seq_length=MAX_SEQ_LEN, \n",
    "                              num_class=Num_Label)\n",
    "model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = LabelSmoothingCrossEntropy()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(), lr=0.00008)\n",
    "scheduler = lr_scheduler.LambdaLR(\n",
    "    optimizer=optimizer, lr_lambda=lambda epoch: 1 / ((epoch/4) + 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(test_df) :\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = PetDataset(test_df)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for text, label in test_loader:\n",
    "        #   encoded_list = [tokenizer.encode(t, add_special_token=True) for t in text]\n",
    "        encoded_list = [tokenizer.encode(t, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(sample=sample)\n",
    "        logits = outputs\n",
    "\n",
    "        pred = torch.argmax(F.softmax(logits), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "\n",
    "    print('Test accuracy: ', total_correct / total_len) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [0][100/585]\tloss 2.76273\n",
      "Train: [0][200/585]\tloss 2.75689\n",
      "Train: [0][300/585]\tloss 2.75086\n",
      "Train: [0][400/585]\tloss 2.75258\n",
      "Train: [0][500/585]\tloss 2.75721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/beomgon2/.local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.1992619926199262\n",
      "***********************************\n",
      "Train: [1][100/585]\tloss 2.55303\n",
      "Train: [1][200/585]\tloss 2.51337\n",
      "Train: [1][300/585]\tloss 2.48054\n",
      "Train: [1][400/585]\tloss 2.47218\n",
      "Train: [1][500/585]\tloss 2.45695\n",
      "Test accuracy:  0.22755227552275523\n",
      "***********************************\n",
      "Train: [2][100/585]\tloss 2.17318\n",
      "Train: [2][200/585]\tloss 2.20296\n",
      "Train: [2][300/585]\tloss 2.19177\n",
      "Train: [2][400/585]\tloss 2.15886\n",
      "Train: [2][500/585]\tloss 2.13957\n",
      "Test accuracy:  0.42435424354243545\n",
      "***********************************\n",
      "Train: [3][100/585]\tloss 1.83167\n",
      "Train: [3][200/585]\tloss 1.80540\n",
      "Train: [3][300/585]\tloss 1.79732\n",
      "Train: [3][400/585]\tloss 1.77206\n",
      "Train: [3][500/585]\tloss 1.77757\n",
      "Test accuracy:  0.4489544895448955\n",
      "***********************************\n",
      "Train: [4][100/585]\tloss 1.55736\n",
      "Train: [4][200/585]\tloss 1.54889\n",
      "Train: [4][300/585]\tloss 1.54460\n",
      "Train: [4][400/585]\tloss 1.56000\n",
      "Train: [4][500/585]\tloss 1.56283\n",
      "Test accuracy:  0.45264452644526443\n",
      "***********************************\n",
      "Train: [5][100/585]\tloss 1.40821\n",
      "Train: [5][200/585]\tloss 1.38849\n",
      "Train: [5][300/585]\tloss 1.39172\n",
      "Train: [5][400/585]\tloss 1.40426\n",
      "Train: [5][500/585]\tloss 1.39975\n",
      "Test accuracy:  0.43665436654366546\n",
      "***********************************\n",
      "Train: [6][100/585]\tloss 1.25886\n",
      "Train: [6][200/585]\tloss 1.25569\n",
      "Train: [6][300/585]\tloss 1.27047\n",
      "Train: [6][400/585]\tloss 1.28436\n",
      "Train: [6][500/585]\tloss 1.30080\n",
      "Test accuracy:  0.4440344403444034\n",
      "***********************************\n",
      "Test accuracy:  0.4440344403444034\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 7\n",
    "for epoch in range(epochs):\n",
    "    losses = AverageMeter()\n",
    "    total_loss = 0\n",
    "    total_len = 0\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    model.train()    \n",
    "    for text, label in train_loader:\n",
    "        encoded_list = [tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True) for t in text]\n",
    "        padded_list = [e[:512] + [0] * (512-len(e[:512])) for e in encoded_list]\n",
    "        sample = torch.tensor(padded_list)\n",
    "        sample, label = sample.to(device), label.to(device)\n",
    "        label = torch.tensor(label)\n",
    "        outputs = model(sample=sample)\n",
    "        pred = torch.argmax(F.softmax(outputs), dim=1)\n",
    "        correct = pred.eq(label)\n",
    "        loss = criterion(outputs, label)\n",
    "        losses.update(loss.item(), BATCH_SIZE)\n",
    "#         print(loss)\n",
    "        \n",
    "        total_correct += correct.sum().item()\n",
    "        total_len += len(label)\n",
    "        total_loss += loss.item()\n",
    "        total_count += 1\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "\n",
    "        if (total_count + 1) % 100 == 0:\n",
    "            print('Train: [{0}][{1}/{2}]\\t'\n",
    "                  'loss {loss.avg:.5f}'.format(\n",
    "                   epoch, total_count + 1, len(train_loader), loss=losses))   \n",
    "\n",
    "    model.train()\n",
    "    scheduler.step()\n",
    "    model_eval(test_df)\n",
    "    print('***********************************')\n",
    "#     print(tloss)     \n",
    "model_eval(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
