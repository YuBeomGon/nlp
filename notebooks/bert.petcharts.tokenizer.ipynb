{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LmTHPnU_4tl8"
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eN1ePry545zM"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('./gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNbjX7XJ5CnS"
   },
   "outputs": [],
   "source": [
    "# cd /content/gdrive/My\\ Drive/DeepLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordpiecepiece tokenizer\n",
    "# https://keep-steady.tistory.com/37\n",
    "# https://monologg.kr/2020/04/27/wordpiece-vocab/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IMnymRDLe0hi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 38s, sys: 10.9 s, total: 5min 49s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "from pathlib import Path\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# paths = [str(x) for x in Path(\"./samples\").glob(\"**/corpus.txt\")]\n",
    "paths = 'files/pet_0814.txt'\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=paths, vocab_size=32000, min_frequency=25, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<s1>\",    \n",
    "    \"<s2>\",      \n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens([\"<s1>\", \"<s2>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# tokenizer = BertWordPieceTokenizer(\n",
    "#     vocab_file=None,\n",
    "#     clean_text=True,\n",
    "#     handle_chinese_chars=False,\n",
    "#     strip_accents=False, # Must be False if cased model\n",
    "#     lowercase=False,\n",
    "#     wordpieces_prefix=\"##\"\n",
    "# )\n",
    "\n",
    "# tokenizer.train(\n",
    "#     files='files/pet_0814.txt',\n",
    "#     limit_alphabet=6000,\n",
    "#     vocab_size=32000,\n",
    "#     special_tokens=['[PAD]', '[UNK]', '[CLS]','[CLS1]','[CLS2]', '[SEP]', '[MASK]']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ìķĪë', 'ħķ', 'Ġë°ĺ', 'ê°Ģì', 'ĽĮ', 'Ġmy', 'Ġpet', 'Ġis', 'Ġh', 'ur', 'ted']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('안녕 반가워 my pet is hurted').tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EIS-irI0f32P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘tokenizer’: File exists\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./pretrained_local/vocab.json', './pretrained_local/merges.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !mkdir tokenizer\n",
    "tokenizer.save_model(\"./pretrained_local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "bert.petcharts.tokenizer.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/huggingface/blog/blob/master/notebooks/01_how_to_train.ipynb",
     "timestamp": 1591408632983
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
